{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "582acc03-b9ad-4194-9e87-f450de1e69f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124208, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-21T19:52:45</td>\n",
       "      <td>189292</td>\n",
       "      <td>rate</td>\n",
       "      <td>casino+jack+and+the+united+states+of+money+2010</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-21T19:52:42</td>\n",
       "      <td>200691</td>\n",
       "      <td>rate</td>\n",
       "      <td>quick+change+1990</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0       1     2  \\\n",
       "0  2022-09-21T19:52:45  189292  rate   \n",
       "1  2022-09-21T19:52:42  200691  rate   \n",
       "\n",
       "                                                 3  4  \n",
       "0  casino+jack+and+the+united+states+of+money+2010  2  \n",
       "1                                quick+change+1990  3  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np, pandas as pd\n",
    "import pickle, gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "movies_pivot = pd.read_csv('data/kafka_log-movielog6_stream_processed(initial).csv', header=None)\n",
    "print(movies_pivot.shape)\n",
    "movies_pivot.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aad5a395-a199-4f3f-8a6e-b4a9c90e4a92",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"model/top_k_movies.pkl\",'wb') as f:\n",
    "    top_k_movies = movies_pivot.sort_values(4, ascending=False)[3].tolist()\n",
    "    pickle.dump(top_k_movies, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a59a3648-6323-4350-b1cc-b750d11ddbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67786, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>peter+pan+1953</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ruhr+2009</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         movie_id  rate\n",
       "1                      \n",
       "2  peter+pan+1953   4.0\n",
       "3       ruhr+2009   3.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"creating a pivot table\"\"\"\n",
    "movies_pivot = movies_pivot.groupby(1).aggregate({3:'first', 4: 'mean'})\n",
    "movies_pivot.columns = ['movie_id', 'rate']\n",
    "print(movies_pivot.shape)\n",
    "movies_pivot.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da465cf1-e640-4f31-9eab-36c2f1dbb37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10492</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11463</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         movie_id  rate\n",
       "user_id                \n",
       "0           10492   4.0\n",
       "1           11463   3.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_label = LabelEncoder().fit_transform(movies_pivot.index)\n",
    "movie_label = LabelEncoder().fit_transform(movies_pivot.movie_id)\n",
    "mappings = {\n",
    "    'user': {k: v for k,v in zip(user_label, movies_pivot.index)},\n",
    "    'movie': {k: v for k,v in zip(movie_label, movies_pivot['movie_id'])}\n",
    "}\n",
    "with open(\"model/id_mapping.pkl\",'wb') as f:\n",
    "    pickle.dump(mappings, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#\n",
    "movies_pivot= movies_pivot.reset_index(drop=True)\n",
    "movies_pivot.index.name = 'user_id'\n",
    "movies_pivot['movie_id'] = movie_label\n",
    "movies_pivot.to_csv(\"data/user_movie_rating.csv\")\n",
    "movies_pivot.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd66332-70f8-4969-8843-45b76cc3ad3f",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec3cfb99-6de3-45b9-aea0-4564f09ba9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import MatrixFactorization\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8393f5fe-fb04-4770-9656-c40779cfccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieRating(Dataset):\n",
    " \n",
    "    def __init__(self, file_name=\"data/user_movie_features.csv\"):\n",
    "        df=pd.read_csv(file_name)\n",
    "\n",
    "        x = df[['user_id','movie_id']].values\n",
    "        y = df['rate'].values\n",
    "        self.n_user = df['user_id'].nunique()\n",
    "        self.n_movie = df['movie_id'].nunique()\n",
    "        self.x_train=torch.tensor(x, dtype=torch.long)\n",
    "        self.y_train=torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx], self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee221ad6-e760-47b1-b986-bfdff97cb568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "dataset = MovieRating('data/user_movie_rating.csv')\n",
    "dataloader = DataLoader(dataset, batch_size=128)\n",
    "model = MatrixFactorization(dataset.n_user, dataset.n_movie, n_factors=20)\n",
    "loss_fn = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88c48f64-076c-4620-b893-4e05208bec8d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Batch 0: loss:29.855697631835938\n",
      "Batch 20: loss:33.918785095214844\n",
      "Batch 40: loss:30.947935104370117\n",
      "Batch 60: loss:33.46770095825195\n",
      "Batch 80: loss:31.281450271606445\n",
      "Batch 100: loss:28.151803970336914\n",
      "Batch 120: loss:44.713321685791016\n",
      "Batch 140: loss:34.772544860839844\n",
      "Batch 160: loss:37.8205451965332\n",
      "Batch 180: loss:27.98522186279297\n",
      "Batch 200: loss:36.814205169677734\n",
      "Batch 220: loss:28.732946395874023\n",
      "Batch 240: loss:32.466651916503906\n",
      "Batch 260: loss:30.418352127075195\n",
      "Batch 280: loss:41.42574691772461\n",
      "Batch 300: loss:35.4355354309082\n",
      "Batch 320: loss:33.94124221801758\n",
      "Batch 340: loss:46.16339111328125\n",
      "Batch 360: loss:33.44694519042969\n",
      "Batch 380: loss:35.7506103515625\n",
      "Batch 400: loss:33.844810485839844\n",
      "Batch 420: loss:33.492469787597656\n",
      "Batch 440: loss:28.627086639404297\n",
      "Batch 460: loss:33.78052520751953\n",
      "Batch 480: loss:23.22759246826172\n",
      "Batch 500: loss:34.55928039550781\n",
      "Batch 520: loss:25.939205169677734\n",
      "Batch 0: loss:19.003795623779297\n",
      "Batch 20: loss:22.167936325073242\n",
      "Batch 40: loss:18.928916931152344\n",
      "Batch 60: loss:20.140432357788086\n",
      "Batch 80: loss:18.810388565063477\n",
      "Batch 100: loss:16.746440887451172\n",
      "Batch 120: loss:27.638778686523438\n",
      "Batch 140: loss:20.11565399169922\n",
      "Batch 160: loss:21.96866226196289\n",
      "Batch 180: loss:15.380168914794922\n",
      "Batch 200: loss:21.25259780883789\n",
      "Batch 220: loss:15.752471923828125\n",
      "Batch 240: loss:18.544431686401367\n",
      "Batch 260: loss:16.626344680786133\n",
      "Batch 280: loss:24.34333038330078\n",
      "Batch 300: loss:19.012710571289062\n",
      "Batch 320: loss:18.450511932373047\n",
      "Batch 340: loss:26.428865432739258\n",
      "Batch 360: loss:18.211990356445312\n",
      "Batch 380: loss:19.263370513916016\n",
      "Batch 400: loss:17.91295051574707\n",
      "Batch 420: loss:17.689552307128906\n",
      "Batch 440: loss:14.738494873046875\n",
      "Batch 460: loss:17.3919677734375\n",
      "Batch 480: loss:10.66469669342041\n",
      "Batch 500: loss:17.955766677856445\n",
      "Batch 520: loss:13.111912727355957\n",
      "Batch 0: loss:10.32695198059082\n",
      "Batch 20: loss:12.712790489196777\n",
      "Batch 40: loss:9.965605735778809\n",
      "Batch 60: loss:10.49014949798584\n",
      "Batch 80: loss:10.441947937011719\n",
      "Batch 100: loss:9.41600227355957\n",
      "Batch 120: loss:15.607731819152832\n",
      "Batch 140: loss:10.892784118652344\n",
      "Batch 160: loss:11.992585182189941\n",
      "Batch 180: loss:8.230151176452637\n",
      "Batch 200: loss:11.597651481628418\n",
      "Batch 220: loss:8.362120628356934\n",
      "Batch 240: loss:9.964200973510742\n",
      "Batch 260: loss:9.148636817932129\n",
      "Batch 280: loss:14.238526344299316\n",
      "Batch 300: loss:10.02407455444336\n",
      "Batch 320: loss:9.987434387207031\n",
      "Batch 340: loss:14.826606750488281\n",
      "Batch 360: loss:10.16234302520752\n",
      "Batch 380: loss:10.263656616210938\n",
      "Batch 400: loss:9.54604434967041\n",
      "Batch 420: loss:9.479341506958008\n",
      "Batch 440: loss:8.003059387207031\n",
      "Batch 460: loss:9.499390602111816\n",
      "Batch 480: loss:5.035381317138672\n",
      "Batch 500: loss:9.450786590576172\n",
      "Batch 520: loss:7.124547481536865\n",
      "Batch 0: loss:5.799684047698975\n",
      "Batch 20: loss:7.378961563110352\n",
      "Batch 40: loss:5.257974147796631\n",
      "Batch 60: loss:5.499248504638672\n",
      "Batch 80: loss:5.969579696655273\n",
      "Batch 100: loss:5.483828067779541\n",
      "Batch 120: loss:8.804105758666992\n",
      "Batch 140: loss:6.402405738830566\n",
      "Batch 160: loss:7.090925693511963\n",
      "Batch 180: loss:5.010892868041992\n",
      "Batch 200: loss:6.391116142272949\n",
      "Batch 220: loss:4.747087001800537\n",
      "Batch 240: loss:6.015928745269775\n",
      "Batch 260: loss:5.707028865814209\n",
      "Batch 280: loss:8.761469841003418\n",
      "Batch 300: loss:6.053865909576416\n",
      "Batch 320: loss:6.368040084838867\n",
      "Batch 340: loss:8.803642272949219\n",
      "Batch 360: loss:6.229440689086914\n",
      "Batch 380: loss:5.972188472747803\n",
      "Batch 400: loss:5.643983840942383\n",
      "Batch 420: loss:5.858612060546875\n",
      "Batch 440: loss:5.1770830154418945\n",
      "Batch 460: loss:6.086274147033691\n",
      "Batch 480: loss:3.0315144062042236\n",
      "Batch 500: loss:5.681347846984863\n",
      "Batch 520: loss:4.320294380187988\n",
      "Batch 0: loss:3.7542314529418945\n",
      "Batch 20: loss:5.040523052215576\n",
      "Batch 40: loss:3.293402671813965\n",
      "Batch 60: loss:3.8133621215820312\n",
      "Batch 80: loss:3.768263339996338\n",
      "Batch 100: loss:3.6425058841705322\n",
      "Batch 120: loss:5.315799713134766\n",
      "Batch 140: loss:4.33603572845459\n",
      "Batch 160: loss:5.259460926055908\n",
      "Batch 180: loss:3.481682300567627\n",
      "Batch 200: loss:3.9123425483703613\n",
      "Batch 220: loss:3.2245240211486816\n",
      "Batch 240: loss:4.392218112945557\n",
      "Batch 260: loss:4.218132972717285\n",
      "Batch 280: loss:5.7982587814331055\n",
      "Batch 300: loss:4.64879035949707\n",
      "Batch 320: loss:4.77318000793457\n",
      "Batch 340: loss:5.932557582855225\n",
      "Batch 360: loss:4.335943222045898\n",
      "Batch 380: loss:4.271932125091553\n",
      "Batch 400: loss:4.104201793670654\n",
      "Batch 420: loss:4.237757682800293\n",
      "Batch 440: loss:3.964940071105957\n",
      "Batch 460: loss:4.987964630126953\n",
      "Batch 480: loss:2.6485307216644287\n",
      "Batch 500: loss:3.925471544265747\n",
      "Batch 520: loss:3.179201602935791\n",
      "epoch: 5\n",
      "Batch 0: loss:3.4928762912750244\n",
      "Batch 20: loss:4.5598320960998535\n",
      "Batch 40: loss:3.0571165084838867\n",
      "Batch 60: loss:4.090272903442383\n",
      "Batch 80: loss:3.215548038482666\n",
      "Batch 100: loss:3.2506649494171143\n",
      "Batch 120: loss:4.2337188720703125\n",
      "Batch 140: loss:3.8420963287353516\n",
      "Batch 160: loss:5.247625350952148\n",
      "Batch 180: loss:3.1186728477478027\n",
      "Batch 200: loss:3.3785898685455322\n",
      "Batch 220: loss:2.9622743129730225\n",
      "Batch 240: loss:4.0407233238220215\n",
      "Batch 260: loss:3.8606791496276855\n",
      "Batch 280: loss:4.615286350250244\n",
      "Batch 300: loss:4.757279872894287\n",
      "Batch 320: loss:4.46978235244751\n",
      "Batch 340: loss:4.965337753295898\n",
      "Batch 360: loss:3.8823089599609375\n",
      "Batch 380: loss:4.111028671264648\n",
      "Batch 400: loss:3.8049404621124268\n",
      "Batch 420: loss:3.725191354751587\n",
      "Batch 440: loss:3.576761484146118\n",
      "Batch 460: loss:4.720209121704102\n",
      "Batch 480: loss:3.149477005004883\n",
      "Batch 500: loss:3.3857874870300293\n",
      "Batch 520: loss:2.7925877571105957\n",
      "Batch 0: loss:3.7935705184936523\n",
      "Batch 20: loss:4.961275100708008\n",
      "Batch 40: loss:3.861562490463257\n",
      "Batch 60: loss:4.95100212097168\n",
      "Batch 80: loss:3.6639320850372314\n",
      "Batch 100: loss:3.684570074081421\n",
      "Batch 120: loss:4.460608005523682\n",
      "Batch 140: loss:4.5828423500061035\n",
      "Batch 160: loss:5.699505805969238\n",
      "Batch 180: loss:3.609860897064209\n",
      "Batch 200: loss:3.950803756713867\n",
      "Batch 220: loss:3.302807092666626\n",
      "Batch 240: loss:4.3060383796691895\n",
      "Batch 260: loss:4.0924787521362305\n",
      "Batch 280: loss:4.032920837402344\n",
      "Batch 300: loss:5.3608078956604\n",
      "Batch 320: loss:4.82647180557251\n",
      "Batch 340: loss:5.228494644165039\n",
      "Batch 360: loss:4.045151710510254\n",
      "Batch 380: loss:4.360259056091309\n",
      "Batch 400: loss:4.222609996795654\n",
      "Batch 420: loss:3.826249361038208\n",
      "Batch 440: loss:3.6311488151550293\n",
      "Batch 460: loss:4.69704008102417\n",
      "Batch 480: loss:3.7383315563201904\n",
      "Batch 500: loss:3.8055827617645264\n",
      "Batch 520: loss:2.868839979171753\n",
      "Batch 0: loss:4.248838424682617\n",
      "Batch 20: loss:5.327890872955322\n",
      "Batch 40: loss:5.149539947509766\n",
      "Batch 60: loss:6.015496253967285\n",
      "Batch 80: loss:4.496936798095703\n",
      "Batch 100: loss:4.25262451171875\n",
      "Batch 120: loss:5.3624701499938965\n",
      "Batch 140: loss:5.413869857788086\n",
      "Batch 160: loss:5.9908766746521\n",
      "Batch 180: loss:4.2940850257873535\n",
      "Batch 200: loss:4.9652533531188965\n",
      "Batch 220: loss:3.8399434089660645\n",
      "Batch 240: loss:4.307055473327637\n",
      "Batch 260: loss:4.369322299957275\n",
      "Batch 280: loss:3.733884572982788\n",
      "Batch 300: loss:5.8353705406188965\n",
      "Batch 320: loss:4.901226043701172\n",
      "Batch 340: loss:5.799718856811523\n",
      "Batch 360: loss:4.3646626472473145\n",
      "Batch 380: loss:4.539911270141602\n",
      "Batch 400: loss:4.66947078704834\n",
      "Batch 420: loss:4.017786979675293\n",
      "Batch 440: loss:3.8479747772216797\n",
      "Batch 460: loss:4.569601058959961\n",
      "Batch 480: loss:4.222378253936768\n",
      "Batch 500: loss:4.445657730102539\n",
      "Batch 520: loss:3.3723244667053223\n",
      "Batch 0: loss:4.815449237823486\n",
      "Batch 20: loss:5.115241527557373\n",
      "Batch 40: loss:6.063642501831055\n",
      "Batch 60: loss:6.705147743225098\n",
      "Batch 80: loss:5.169790744781494\n",
      "Batch 100: loss:4.559723377227783\n",
      "Batch 120: loss:6.259875297546387\n",
      "Batch 140: loss:5.937924385070801\n",
      "Batch 160: loss:5.870666980743408\n",
      "Batch 180: loss:4.6375203132629395\n",
      "Batch 200: loss:5.66845703125\n",
      "Batch 220: loss:4.392070293426514\n",
      "Batch 240: loss:4.387798309326172\n",
      "Batch 260: loss:4.314333915710449\n",
      "Batch 280: loss:3.9855740070343018\n",
      "Batch 300: loss:5.943725109100342\n",
      "Batch 320: loss:4.6670427322387695\n",
      "Batch 340: loss:6.153722286224365\n",
      "Batch 360: loss:4.5461506843566895\n",
      "Batch 380: loss:4.771897315979004\n",
      "Batch 400: loss:4.747907638549805\n",
      "Batch 420: loss:4.265330791473389\n",
      "Batch 440: loss:4.221848964691162\n",
      "Batch 460: loss:4.222916126251221\n",
      "Batch 480: loss:4.305257320404053\n",
      "Batch 500: loss:4.844161510467529\n",
      "Batch 520: loss:3.742780923843384\n",
      "Batch 0: loss:4.993710994720459\n",
      "Batch 20: loss:4.774871349334717\n",
      "Batch 40: loss:6.251715183258057\n",
      "Batch 60: loss:6.658914089202881\n",
      "Batch 80: loss:5.472494602203369\n",
      "Batch 100: loss:4.658262729644775\n",
      "Batch 120: loss:6.766513824462891\n",
      "Batch 140: loss:5.743389129638672\n",
      "Batch 160: loss:5.327549457550049\n",
      "Batch 180: loss:4.555360794067383\n",
      "Batch 200: loss:5.720542907714844\n",
      "Batch 220: loss:4.667641639709473\n",
      "Batch 240: loss:4.534634590148926\n",
      "Batch 260: loss:3.8282086849212646\n",
      "Batch 280: loss:4.480781078338623\n",
      "Batch 300: loss:5.411573886871338\n",
      "Batch 320: loss:4.30912971496582\n",
      "Batch 340: loss:6.171370506286621\n",
      "Batch 360: loss:4.3976006507873535\n",
      "Batch 380: loss:4.880923271179199\n",
      "Batch 400: loss:4.49910306930542\n",
      "Batch 420: loss:4.375903129577637\n",
      "Batch 440: loss:4.099282264709473\n",
      "Batch 460: loss:3.826359510421753\n",
      "Batch 480: loss:3.6494932174682617\n",
      "Batch 500: loss:4.755566120147705\n",
      "Batch 520: loss:3.6568706035614014\n",
      "epoch: 10\n",
      "Batch 0: loss:4.746099948883057\n",
      "Batch 20: loss:4.407278060913086\n",
      "Batch 40: loss:5.8779683113098145\n",
      "Batch 60: loss:6.059444904327393\n",
      "Batch 80: loss:5.202929973602295\n",
      "Batch 100: loss:4.364827632904053\n",
      "Batch 120: loss:6.79398250579834\n",
      "Batch 140: loss:5.064870834350586\n",
      "Batch 160: loss:4.701901912689209\n",
      "Batch 180: loss:3.940673351287842\n",
      "Batch 200: loss:5.338189125061035\n",
      "Batch 220: loss:4.5052385330200195\n",
      "Batch 240: loss:4.47160005569458\n",
      "Batch 260: loss:3.165754556655884\n",
      "Batch 280: loss:4.85389518737793\n",
      "Batch 300: loss:4.5099029541015625\n",
      "Batch 320: loss:3.8720598220825195\n",
      "Batch 340: loss:5.837409973144531\n",
      "Batch 360: loss:4.012984275817871\n",
      "Batch 380: loss:4.718423366546631\n",
      "Batch 400: loss:4.025756359100342\n",
      "Batch 420: loss:4.176092624664307\n",
      "Batch 440: loss:3.4832277297973633\n",
      "Batch 460: loss:3.410155773162842\n",
      "Batch 480: loss:2.878692865371704\n",
      "Batch 500: loss:4.34174108505249\n",
      "Batch 520: loss:3.0768957138061523\n",
      "Batch 0: loss:4.331130027770996\n",
      "Batch 20: loss:4.004966735839844\n",
      "Batch 40: loss:5.117891311645508\n",
      "Batch 60: loss:5.091116428375244\n",
      "Batch 80: loss:4.420831680297852\n",
      "Batch 100: loss:3.718189239501953\n",
      "Batch 120: loss:6.276349067687988\n",
      "Batch 140: loss:4.462495803833008\n",
      "Batch 160: loss:4.004825115203857\n",
      "Batch 180: loss:3.103337287902832\n",
      "Batch 200: loss:4.786293029785156\n",
      "Batch 220: loss:4.023336887359619\n",
      "Batch 240: loss:4.261178016662598\n",
      "Batch 260: loss:2.7467424869537354\n",
      "Batch 280: loss:4.84107780456543\n",
      "Batch 300: loss:3.589279890060425\n",
      "Batch 320: loss:3.456265687942505\n",
      "Batch 340: loss:5.337488651275635\n",
      "Batch 360: loss:3.352932929992676\n",
      "Batch 380: loss:4.406413555145264\n",
      "Batch 400: loss:3.4892210960388184\n",
      "Batch 420: loss:3.8571040630340576\n",
      "Batch 440: loss:2.870250701904297\n",
      "Batch 460: loss:3.0863101482391357\n",
      "Batch 480: loss:2.3823165893554688\n",
      "Batch 500: loss:3.768059253692627\n",
      "Batch 520: loss:2.367561101913452\n",
      "Batch 0: loss:3.6594061851501465\n",
      "Batch 20: loss:3.6104769706726074\n",
      "Batch 40: loss:4.003580093383789\n",
      "Batch 60: loss:4.174985885620117\n",
      "Batch 80: loss:3.525117874145508\n",
      "Batch 100: loss:3.1821517944335938\n",
      "Batch 120: loss:5.512195587158203\n",
      "Batch 140: loss:4.012565612792969\n",
      "Batch 160: loss:3.359076738357544\n",
      "Batch 180: loss:2.396127462387085\n",
      "Batch 200: loss:4.129520416259766\n",
      "Batch 220: loss:3.3578779697418213\n",
      "Batch 240: loss:3.897427797317505\n",
      "Batch 260: loss:2.493358850479126\n",
      "Batch 280: loss:4.471038341522217\n",
      "Batch 300: loss:3.0387074947357178\n",
      "Batch 320: loss:3.1866157054901123\n",
      "Batch 340: loss:4.922208309173584\n",
      "Batch 360: loss:2.750479221343994\n",
      "Batch 380: loss:3.960209846496582\n",
      "Batch 400: loss:3.122199773788452\n",
      "Batch 420: loss:3.4697909355163574\n",
      "Batch 440: loss:2.33668851852417\n",
      "Batch 460: loss:2.830068826675415\n",
      "Batch 480: loss:2.1142992973327637\n",
      "Batch 500: loss:3.2333824634552\n",
      "Batch 520: loss:1.7948951721191406\n",
      "Batch 0: loss:2.9053683280944824\n",
      "Batch 20: loss:3.1228933334350586\n",
      "Batch 40: loss:3.0314531326293945\n",
      "Batch 60: loss:3.4420735836029053\n",
      "Batch 80: loss:2.786532402038574\n",
      "Batch 100: loss:2.7129769325256348\n",
      "Batch 120: loss:4.566597938537598\n",
      "Batch 140: loss:3.6231677532196045\n",
      "Batch 160: loss:2.943596601486206\n",
      "Batch 180: loss:2.0730814933776855\n",
      "Batch 200: loss:3.4716761112213135\n",
      "Batch 220: loss:2.81894588470459\n",
      "Batch 240: loss:3.325441360473633\n",
      "Batch 260: loss:2.3453209400177\n",
      "Batch 280: loss:3.9452216625213623\n",
      "Batch 300: loss:2.74308443069458\n",
      "Batch 320: loss:3.03901743888855\n",
      "Batch 340: loss:4.545429229736328\n",
      "Batch 360: loss:2.429090976715088\n",
      "Batch 380: loss:3.348839044570923\n",
      "Batch 400: loss:2.8497366905212402\n",
      "Batch 420: loss:3.0383758544921875\n",
      "Batch 440: loss:1.9517549276351929\n",
      "Batch 460: loss:2.6146063804626465\n",
      "Batch 480: loss:1.993830680847168\n",
      "Batch 500: loss:2.7452127933502197\n",
      "Batch 520: loss:1.4695059061050415\n",
      "Batch 0: loss:2.2714149951934814\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/sparse_adam.py:101\u001b[0m, in \u001b[0;36mSparseAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m     99\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                  \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/_functional.py:57\u001b[0m, in \u001b[0;36msparse_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, state_steps, eps, beta1, beta2, lr)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m constructor(grad_indices, values, size)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#      old <- b * old + (1 - b) * new\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# <==> old += (1 - b) * (new - old)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m old_exp_avg_values \u001b[38;5;241m=\u001b[39m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_values()\n\u001b[1;32m     58\u001b[0m exp_avg_update_values \u001b[38;5;241m=\u001b[39m grad_values\u001b[38;5;241m.\u001b[39msub(old_exp_avg_values)\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m     59\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39madd_(make_sparse(exp_avg_update_values))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n_ep in range(20):\n",
    "    if n_ep%5==0: print(f\"epoch: {n_ep}\")\n",
    "    for idx, (data, target) in enumerate(dataloader):\n",
    "        # predict\n",
    "        prediction = model(data)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        if idx%20==0: print(f\"Batch {idx}: loss:{loss:.2f}\")\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26583788-41f9-4700-b144-c6b27169873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_movie_recommendation.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
